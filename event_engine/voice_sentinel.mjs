/**
 * @fileoverview
 * ä¸€ä¸ªåŸºäº PvRecorder çš„æ™ºèƒ½è¯­éŸ³å“¨å…µæ¨¡å—ã€‚
 * å½“ä¾¦æµ‹åˆ°å£°éŸ³æ—¶ï¼Œä¼šå°†å…¶å½•åˆ¶ä¸‹æ¥ã€‚å¦‚æœå£°éŸ³ä¸ä¸»äººéŸ³è‰²åŒ¹é…ï¼Œåˆ™ä¼šè§¦å‘AIè¿›è¡Œå›åº”ã€‚
 */

import { Buffer } from 'node:buffer'
import { existsSync, readFileSync, statSync } from 'node:fs'
import { join } from 'node:path'

import { PvRecorder } from 'npm:@picovoice/pvrecorder-node'
import Meyda from 'npm:meyda'
import wavefile from 'npm:wavefile'
import 'npm:@steve02081504/virtual-console'

import { chardir, charname } from '../charbase.mjs'
import { config as charConfig } from '../config/index.mjs'
import { GetReply } from '../reply_gener/index.mjs'

import { initRealityChannel, RealityChannel } from './index.mjs'

// --- é…ç½®ä¸­å¿ƒ (Configuration) ---
const CONFIG = {
	// --- é˜ˆå€¼ä¸è§¦å‘å™¨ (Thresholds & Triggers) ---
	thresholds: {
		QUIET_STD_MULTIPLIER: 1.0,  // å®‰é™é˜ˆå€¼ = å¹³å‡RMS + (æ ‡å‡†å·® * N)ã€‚
		LOUD_STD_MULTIPLIER: 3.5,   // å“äº®é˜ˆå€¼ = å¹³å‡RMS + (æ ‡å‡†å·® * N)ã€‚
		MIN_QUIET_THRESHOLD: 50,    // åŠ¨æ€è®¡ç®—åçš„æœ€ä½å®‰é™é˜ˆå€¼ï¼Œé˜²æ­¢åœ¨æé™ç¯å¢ƒä¸‹è¿‡äºæ•æ„Ÿã€‚
		MIN_LOUD_THRESHOLD: 100,    // åŠ¨æ€è®¡ç®—åçš„æœ€ä½å“äº®é˜ˆå€¼ã€‚
	},
	triggers: {
		CONSECUTIVE_LOUD_FRAMES: 4, // è§¦å‘å½•éŸ³æ‰€éœ€çš„ã€è¿ç»­ã€‘å“äº®å¸§æ•°ã€‚
		PRE_BUFFER_FRAME_COUNT: 15, // é¢„ç¼“å†²çš„å¸§æ•°ã€‚
		SILENCE_TAIL_FRAME_COUNT: 3, // å½•éŸ³æœ«å°¾ä¿ç•™çš„é™éŸ³å¸§æ•°ã€‚
		LOUD_ACTIVITY_THRESHOLD: 0.4, // å½“è¿‘æœŸå“äº®å¸§æ¯”ä¾‹è¶…è¿‡æ­¤å€¼æ—¶ï¼Œè½¬ä¸ºé™é»˜ç›‘æµ‹ã€‚
	},
	// --- æ—¶é—´ä¸æŒç»­æ€§ (Timing & Duration) ---
	timing: {
		INITIALIZATION_MS: 10 * 1000,    // ç¯å¢ƒå™ªéŸ³åˆ†ææ—¶é•¿ã€‚
		QUIET_PERIOD_FOR_ARMING_MS: 2.5 * 1000, // ä»â€œç›‘æ§ä¸­â€åˆ‡æ¢åˆ°â€œæ­¦è£…â€æ‰€éœ€çš„é™éŸ³æ—¶é•¿ã€‚
		SILENCE_TIMEOUT_MS: 1.5 * 1000,      // å½•éŸ³æœŸé—´ï¼Œç»“æŸå½•éŸ³çš„é™éŸ³è¶…æ—¶ã€‚
		MAX_RECORDING_MS: 3 * 60 * 1000,    // å•æ¬¡å½•éŸ³æœ€å¤§æ—¶é•¿ã€‚
		MIN_RECORDING_MS: 2 * 1000,         // ä¿®å‰ªåï¼Œå½•éŸ³çš„æœ€å°æ—¶é•¿ã€‚
		IN_RECORDING_VALIDATION_INTERVAL_MS: 1000, // å½•éŸ³ä¸­é€”éªŒè¯é—´éš”ã€‚
		LOUD_ACTIVITY_WINDOW_MS: 90 * 1000, // ç”¨äºè®¡ç®—å“äº®å¸§æ¯”ä¾‹çš„æ—¶é—´çª—å£ã€‚
	},
	// --- æ–‡ä»¶ä¸æ ¼å¼ (File & Format) ---
	files: {
		REFERENCE_WAV: join(chardir, 'vars/master-voice-reference.wav'),
	},
	audio: {
		FRAME_LENGTH: 512,
		SAMPLE_RATE: 16000,
		CHANNELS: 1,
	},
	// --- éŸ³è‰²åŒ¹é… (Voice Matching) ---
	voiceMatch: {
		MFCC_SIMILARITY_THRESHOLD: 0.7,   // 0.7 è¾ƒå®½æ¾ï¼Œ0.8-0.85 æ›´ä¸¥æ ¼ã€‚
		MATCHING_RATIO_THRESHOLD: 0.72,   // å½•éŸ³ä¸­ï¼Œæœ‰å£°å¸§åŒ¹é…éŸ³è‰²çš„æœ€ä½æ¯”ä¾‹ã€‚
		REFERENCE_RMS_MULTIPLIER: 0.5,    // æå–å‚è€ƒéŸ³è‰²æ—¶ï¼Œä½¿ç”¨ (å®‰é™é˜ˆå€¼ * N) ä½œä¸ºæœ‰æ•ˆå£°éŸ³é—¨æ§›ã€‚
	},
	// --- æ˜¾ç¤ºé…ç½® (Display) ---
	display: {
		CLI_BAR_MAX_LENGTH: 50,
		CLI_BAR_RMS_SCALING_FACTOR: 30,
	},
}

// --- çŠ¶æ€ç®¡ç†ä¸­å¿ƒ (State Management) ---

function createInitialRecordingStats() {
	return {
		totalRms: 0, frameCount: 0, totalLoudFrames: 0, matchingLoudFrames: 0,
		longestInternalSilenceFrames: 0, // è®°å½•åˆ°ç›®å‰ä¸ºæ­¢çš„æœ€é•¿è¿ç»­é™éŸ³å¸§æ•°
		currentSilenceStreakFrames: 0,   // å½“å‰æ­£åœ¨æŒç»­çš„è¿ç»­é™éŸ³å¸§æ•°
	}
}

function createInitialState() {
	return {
		state: 'INITIALIZING', // INITIALIZING, MONITORING_QUIET, ARMED, RECORDING
		recorder: null,
		referenceMfccs: null,
		referenceFileMtime: null,
		dynamicThresholds: {
			quiet: CONFIG.thresholds.MIN_QUIET_THRESHOLD,
			loud: CONFIG.thresholds.MIN_LOUD_THRESHOLD,
		},
		initRmsList: [],
		quietStartTime: null,
		lastLoudTime: null,
		armingBuffer: [],
		recordingBuffer: [],
		recordingStartTime: null,
		consecutiveLoudFrames: 0,
		lastValidationCheckTime: 0,
		activityLog: [],
		currentRecordingStats: createInitialRecordingStats(),
		recorderRetryCount: 0,
		avgEnvRms: 0,
	}
}

let sentinelState = createInitialState()

// --- å·¥å…·å‡½æ•° (Utility Functions) ---

const calculateRMS = int16Array => Math.sqrt(int16Array.reduce((sum, val) => sum + val * val, 0) / int16Array.length)
const int16ToFloat32 = int16Array => Float32Array.from(int16Array, v => v / 32768)
const calculateMean = data => data.reduce((sum, value) => sum + value, 0) / data.length
const calculateStdDev = (data, mean) => Math.sqrt(calculateMean(data.map(value => (value - mean) ** 2)))

function cosineSimilarity(vecA, vecB) {
	let dotProduct = 0, normA = 0, normB = 0
	// ä» i = 1 å¼€å§‹ï¼Œå¿½ç•¥ç¬¬ä¸€ä¸ªä¸»è¦åæ˜ éŸ³é‡å¤§å°çš„MFCCç³»æ•°(C0)ï¼Œä¸“æ³¨äºéŸ³è‰²æœ¬èº«çš„æ¯”è¾ƒ
	const len = Math.min(vecA.length, vecB.length)
	for (let i = 1; i < len; i++) {
		dotProduct += vecA[i] * vecB[i]
		normA += vecA[i] ** 2
		normB += vecB[i] ** 2
	}
	return normA === 0 || normB === 0 ? 0 : dotProduct / (Math.sqrt(normA) * Math.sqrt(normB))
}

/**
 * æ£€æŸ¥å½“å‰å¸§æ˜¯å¦ç¬¦åˆå‚è€ƒéŸ³è‰²ã€‚
 * @param {Int16Array} frame - å½“å‰å¸§çš„ Int16 æ•°æ®
 * @returns {boolean} - æ˜¯å¦ç¬¦åˆéŸ³è‰²
 */
function isFrameMatchingVoice(frame) {
	if (!sentinelState.referenceMfccs || sentinelState.referenceMfccs.length === 0) return false
	const frameFloat = int16ToFloat32(frame)
	const mfcc = Meyda.extract('mfcc', frameFloat)
	if (!mfcc) return false

	let maxSimilarity = 0
	for (const refMfcc of sentinelState.referenceMfccs)
		maxSimilarity = Math.max(maxSimilarity, cosineSimilarity(mfcc, refMfcc))

	return maxSimilarity > CONFIG.voiceMatch.MFCC_SIMILARITY_THRESHOLD
}

// --- æ–‡ä»¶ä¸åæœŸå¤„ç† ---

function loadReferenceMfcc() {
	const refPath = CONFIG.files.REFERENCE_WAV
	if (!existsSync(refPath)) {
		console.error(`âŒ Reference audio file does not exist: ${refPath}.`)
		sentinelState.referenceFileMtime = null
		return null
	}
	try {
		const refWav = new wavefile.WaveFile(readFileSync(refPath))

		// --- éªŒè¯éŸ³é¢‘æ ¼å¼ ---
		if (refWav.fmt.sampleRate !== CONFIG.audio.SAMPLE_RATE) {
			console.error(`âŒ Reference audio sample rate mismatch. Requires ${CONFIG.audio.SAMPLE_RATE} Hz, but file is ${refWav.fmt.sampleRate} Hz.`)
			return null
		}
		if (refWav.fmt.numChannels !== CONFIG.audio.CHANNELS) {
			console.error(`âŒ Reference audio channel count mismatch. Requires ${CONFIG.audio.CHANNELS} (mono), but file is ${refWav.fmt.numChannels} channels.`)
			return null
		}

		let refSamples
		const allSamples = refWav.getSamples(false, Int16Array)

		if (refWav.fmt.numChannels > 1)
			// ç«‹ä½“å£°æ–‡ä»¶ï¼ŒæŒ‰åŸé€»è¾‘å–ç¬¬ä¸€ä¸ªå£°é“
			refSamples = allSamples[0]
		else
			// å•å£°é“æ–‡ä»¶ï¼Œè¿”å›çš„å°±æ˜¯æ ·æœ¬æ•°ç»„æœ¬èº«
			refSamples = allSamples

		const mfccs = []

		// --- è‡ªé€‚åº”é˜ˆå€¼è®¡ç®— ---
		// 1. è®¡ç®—å‚è€ƒéŸ³é¢‘æ¯ä¸€å¸§çš„ RMS
		const frames = []
		for (let i = 0; i + CONFIG.audio.FRAME_LENGTH <= refSamples.length; i += CONFIG.audio.FRAME_LENGTH) {
			const buffer = refSamples.subarray(i, i + CONFIG.audio.FRAME_LENGTH)
			frames.push({ buffer, rms: calculateRMS(buffer) })
		}

		if (frames.length === 0) {
			console.error('âŒ Reference audio file contains no valid audio frames (file might be too short).')
			return null
		}

		// 2. åŸºäº RMS åˆ†å¸ƒè®¡ç®—ä¸€ä¸ªåŠ¨æ€é˜ˆå€¼ï¼Œä»¥åŒºåˆ†è¯­éŸ³å’Œé™éŸ³
		const frameRmsValues = frames.map(f => f.rms)
		const avgRefRms = calculateMean(frameRmsValues)
		const stdDevRefRms = calculateStdDev(frameRmsValues, avgRefRms)
		// å°†é«˜äº (å¹³å‡å€¼ + 0.5 * æ ‡å‡†å·®) çš„å¸§è§†ä¸ºæœ‰æ•ˆè¯­éŸ³
		const referenceRmsThreshold = avgRefRms + stdDevRefRms * 0.5

		// 3. ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼æå– MFCC
		for (const frame of frames)
			if (frame.rms > referenceRmsThreshold) {
				const mfcc = Meyda.extract('mfcc', int16ToFloat32(frame.buffer))
				if (mfcc) mfccs.push(mfcc)
			}

		if (mfccs.length === 0) {
			console.error('âŒ Failed to extract MFCC from reference audio. Please check file content and volume (file might be too quiet or only contains noise).')
			return null
		}
		console.log(`ğŸ“Š Reference voice MFCC feature set loaded (${mfccs.length} feature vectors).`)
		const { mtime } = statSync(refPath)
		sentinelState.referenceFileMtime = mtime
		return mfccs
	} catch (err) {
		console.error(`âŒ Failed to load reference audio: ${err.message}`)
		return null
	}
}

async function finishRecordingSession(now) {
	console.log('ğŸ Recording session ended, performing final validation...')
	const { currentRecordingStats, referenceMfccs, recordingBuffer } = sentinelState

	// --- ç®€åŒ–è£å‰ªé€»è¾‘ ---
	// åŠ¨æ€è¶…æ—¶å·²åœ¨ handleRecordingState ä¸­å¤„ç†ï¼Œè¿™é‡Œåªéœ€è¿›è¡Œæœ€åçš„è£å‰ªã€‚
	// æˆ‘ä»¬æ‰¾åˆ°æœ€åä¸€ä¸ªæœ‰æ•ˆå£°éŸ³å¸§ï¼Œå¹¶åœ¨å…¶åä¿ç•™ä¸€å°æ®µå›ºå®šçš„é™éŸ³ä½œä¸ºç¼“å†²ã€‚
	const lastLoudFrameIndex = recordingBuffer.findLastIndex(f => f.rms > sentinelState.dynamicThresholds.quiet)
	const silenceTailFrameCount = CONFIG.triggers.SILENCE_TAIL_FRAME_COUNT // ä½¿ç”¨é…ç½®ä¸­å®šä¹‰çš„å›ºå®šå°¾å·´

	console.log(`âœ‚ï¸  Trimming recording, keeping ${silenceTailFrameCount} frames as silence tail.`)

	const trimmedFramesRaw = lastLoudFrameIndex === -1 ? [] : recordingBuffer.slice(0, Math.min(
		lastLoudFrameIndex + 1 + silenceTailFrameCount,
		recordingBuffer.length,
	))

	const trimmedDurationMs = (trimmedFramesRaw.length * CONFIG.audio.FRAME_LENGTH / CONFIG.audio.SAMPLE_RATE) * 1000
	if (trimmedDurationMs < CONFIG.timing.MIN_RECORDING_MS) {
		console.log(`ğŸ—‘ï¸  Final validation failed: trimmed duration (${trimmedDurationMs.toFixed(0)}ms) too short.`)
		transitionToState('ARMED', now)
		return
	}

	const matchingRatio = currentRecordingStats.totalLoudFrames > 0
		? currentRecordingStats.matchingLoudFrames / currentRecordingStats.totalLoudFrames : 0
	const isMatch = referenceMfccs && matchingRatio >= CONFIG.voiceMatch.MATCHING_RATIO_THRESHOLD

	console.log(`â„¹ï¸  Recording validation complete. Voice match ratio: ${matchingRatio.toFixed(2)}.`)

	// --- å°†å½•éŸ³æ·»åŠ åˆ°ç°å®é¢‘é“ ---
	const trimmedFrames = trimmedFramesRaw.map(f => f.buffer)
	const totalLength = trimmedFrames.reduce((sum, arr) => sum + arr.length, 0)
	const totalSamples = new Int16Array(totalLength)
	let offset = 0
	for (const int16Array of trimmedFrames) {
		totalSamples.set(int16Array, offset)
		offset += int16Array.length
	}

	const wav = new wavefile.WaveFile()
	wav.fromScratch(CONFIG.audio.CHANNELS, CONFIG.audio.SAMPLE_RATE, '16', totalSamples)
	const audioBuffer = Buffer.from(wav.toBuffer().buffer)
	const logEntry = {
		name: 'system',
		role: 'system',
		content: isMatch
			? 'æ£€æµ‹åˆ°å¯èƒ½ä¸ä¸»äººçš„å£°éŸ³ç›¸ä¼¼çš„å£°éŸ³ï¼Œè¯·æ±‚è¯†åˆ«å’Œå¤„ç†ã€‚'
			: 'æ£€æµ‹åˆ°æ¥è‡ªæœªçŸ¥æ¥æºçš„å£°éŸ³ï¼Œå·²è®°å½•ã€‚',
		files: [{
			name: `voice-recording-${new Date().toISOString()}.wav`,
			mime_type: 'audio/wav',
			buffer: audioBuffer
		}],
		charVisibility: [charname],
	}

	if (isMatch) {
		console.log('ğŸ¤ Voice matched! Triggering AI...')
		try {
			const result = await GetReply({
				...RealityChannel,
				chat_log: [...RealityChannel.chat_log, logEntry],
				extension: {
					...RealityChannel.extension,
					is_internal: true,
					enable_prompts: {
						CodeRunner: true,
						fileChange: true,
					}
				}
			})
			result.logContextBefore.push(logEntry)
			await RealityChannel.AddChatLogEntry(result)
		} catch (err) {
			console.error('ğŸ¤ Error during AI reply:', err)
		}
	} else {
		console.log('ğŸ¤ Voice not matched. Only recorded.')
		await RealityChannel.AddChatLogEntry(logEntry)
	}

	transitionToState('ARMED', now)
}

// --- æ ¸å¿ƒçŠ¶æ€æœºé€»è¾‘ (State Machine Logic) ---

function transitionToState(newState, now) {
	const oldState = sentinelState.state
	if (oldState === newState) return

	sentinelState.state = newState
	console.log(`ğŸš¦ State change: ${oldState} -> ${newState}`)

	switch (newState) {
		case 'ARMED':
			sentinelState.quietStartTime = null
			sentinelState.lastLoudTime = null
			sentinelState.recordingBuffer = []
			sentinelState.armingBuffer = []
			sentinelState.recordingStartTime = null
			sentinelState.consecutiveLoudFrames = 0
			sentinelState.lastValidationCheckTime = 0
			sentinelState.activityLog = []
			sentinelState.currentRecordingStats = createInitialRecordingStats()
			break
		case 'MONITORING_QUIET':
			sentinelState.quietStartTime = now
			break
		case 'RECORDING':
			sentinelState.recordingStartTime = now
			sentinelState.lastLoudTime = now
			sentinelState.lastValidationCheckTime = now
			for (const frame of sentinelState.armingBuffer)
				processFrameForRecording(frame)

			sentinelState.armingBuffer = []
			break
		case 'INITIALIZING':
			sentinelState.initRmsList = []
			break
	}
}

function processFrameForRecording(frameData) {
	sentinelState.recordingBuffer.push(frameData)
	const stats = sentinelState.currentRecordingStats
	stats.totalRms += frameData.rms
	stats.frameCount++

	if (frameData.rms > sentinelState.dynamicThresholds.quiet) {
		stats.totalLoudFrames++
		if (isFrameMatchingVoice(frameData.buffer))
			stats.matchingLoudFrames++
	}
}

function handleInitializingState(now) {
	if (now - (sentinelState.initStartTime || (sentinelState.initStartTime = now)) >= CONFIG.timing.INITIALIZATION_MS) {
		const avgRms = calculateMean(sentinelState.initRmsList)
		sentinelState.avgEnvRms = avgRms
		const stdDev = calculateStdDev(sentinelState.initRmsList, avgRms)
		sentinelState.dynamicThresholds.quiet = Math.max(CONFIG.thresholds.MIN_QUIET_THRESHOLD, avgRms + stdDev * CONFIG.thresholds.QUIET_STD_MULTIPLIER)
		sentinelState.dynamicThresholds.loud = Math.max(CONFIG.thresholds.MIN_LOUD_THRESHOLD, avgRms + stdDev * CONFIG.thresholds.LOUD_STD_MULTIPLIER)

		console.log('ğŸ“ Dynamic thresholds calculated:')
		console.log(`   Average RMS: ${avgRms.toFixed(2)} | Standard Deviation: ${stdDev.toFixed(2)}`)
		console.log(`   Quiet Threshold: ${sentinelState.dynamicThresholds.quiet.toFixed(0)}`)
		console.log(`   Loud Threshold: ${sentinelState.dynamicThresholds.loud.toFixed(0)}`)

		sentinelState.referenceMfccs = loadReferenceMfcc()
		transitionToState('ARMED', now)
	}
}

function handleMonitoringQuietState(rms, now) {
	if (rms > sentinelState.dynamicThresholds.quiet)
		sentinelState.quietStartTime = now
	else if (now - sentinelState.quietStartTime >= CONFIG.timing.QUIET_PERIOD_FOR_ARMING_MS)
		transitionToState('ARMED', now)

}

function handleArmedState(frameData, now) {
	const isLoud = frameData.rms > sentinelState.dynamicThresholds.loud

	// --- è¿‘æœŸé«˜æ¿€æ´»æ£€æµ‹ ---
	sentinelState.activityLog.push({ timestamp: now, isLoud })
	const windowStart = now - CONFIG.timing.LOUD_ACTIVITY_WINDOW_MS
	// æ¸…ç†è¶…å‡ºæ—¶é—´çª—å£çš„æ—§æ•°æ®
	while (sentinelState.activityLog.length > 0 && sentinelState.activityLog[0].timestamp < windowStart)
		sentinelState.activityLog.shift()

	// ä»…åœ¨æœ‰è¶³å¤Ÿæ—¶é—´è·¨åº¦çš„æ•°æ®æ—¶æ‰è¿›è¡Œè®¡ç®—ï¼Œé¿å…çª—å£åˆæœŸè¯¯åˆ¤
	const activityDuration = now - (sentinelState.activityLog[0]?.timestamp || now)
	if (activityDuration > CONFIG.timing.LOUD_ACTIVITY_WINDOW_MS / 2) {
		const loudFramesInWindow = sentinelState.activityLog.filter(f => f.isLoud).length
		const loudPercentage = loudFramesInWindow / sentinelState.activityLog.length
		if (loudPercentage > CONFIG.triggers.LOUD_ACTIVITY_THRESHOLD) {
			console.log(`â³ Recent activity ratio too high (${(loudPercentage * 100).toFixed(1)}%), switching to silent monitoring to avoid continuous false triggers.`)
			transitionToState('MONITORING_QUIET', now)
			return
		}
	}

	sentinelState.armingBuffer.push(frameData)
	if (sentinelState.armingBuffer.length > CONFIG.triggers.PRE_BUFFER_FRAME_COUNT)
		sentinelState.armingBuffer.shift()

	if (isLoud) {
		sentinelState.consecutiveLoudFrames++
		if (sentinelState.consecutiveLoudFrames >= CONFIG.triggers.CONSECUTIVE_LOUD_FRAMES) {
			console.log('ğŸ’¥ Detected continuous high volume! Starting recording...')
			transitionToState('RECORDING', now)
		}
	} else
		sentinelState.consecutiveLoudFrames = 0

}

async function handleRecordingState(frameData, now) {
	processFrameForRecording(frameData)
	const stats = sentinelState.currentRecordingStats

	// --- å®æ—¶è¿½è¸ªå†…éƒ¨é™éŸ³ ---
	if (frameData.rms > sentinelState.dynamicThresholds.quiet) {
		// ä¾¦æµ‹åˆ°å£°éŸ³ï¼Œæ„å‘³ç€é™éŸ³ä¸­æ–­
		sentinelState.lastLoudTime = now

		// å¦‚æœåˆšåˆšç»“æŸäº†ä¸€æ®µé™éŸ³ï¼Œæ£€æŸ¥å®ƒæ˜¯å¦æ˜¯å²ä¸Šæœ€é•¿çš„
		if (stats.currentSilenceStreakFrames > 0) {
			stats.longestInternalSilenceFrames = Math.max(
				stats.longestInternalSilenceFrames,
				stats.currentSilenceStreakFrames,
			)
			// é‡ç½®å½“å‰é™éŸ³è®¡æ•°
			stats.currentSilenceStreakFrames = 0
		}
	} else
		// è¿˜åœ¨é™éŸ³ä¸­ï¼Œç´¯åŠ è®¡æ•°
		stats.currentSilenceStreakFrames++


	// --- åŠ¨æ€è®¡ç®—é™éŸ³è¶…æ—¶æ—¶é•¿ ---
	const frameDurationMs = (CONFIG.audio.FRAME_LENGTH / CONFIG.audio.SAMPLE_RATE) * 1000
	// åŠ¨æ€å»¶æ—¶ = å·²çŸ¥çš„æœ€é•¿åœé¡¿ * 1.5
	const dynamicTimeoutExtensionMs = stats.longestInternalSilenceFrames * frameDurationMs * 1.5
	// æœ‰æ•ˆè¶…æ—¶ = max(åŸºç¡€è¶…æ—¶, åŠ¨æ€å»¶æ—¶)ï¼ŒåŒæ—¶è®¾ç½®ä¸€ä¸ªä¸Šé™é˜²æ­¢æ— é™ç­‰å¾…
	const effectiveTimeoutMs = Math.min(
		Math.max(CONFIG.timing.SILENCE_TIMEOUT_MS, dynamicTimeoutExtensionMs),
		7000, // è¶…æ—¶ä¸Šé™ä¸º 7 ç§’ï¼Œå¯æŒ‰éœ€è°ƒæ•´
	)

	// --- ä½¿ç”¨åŠ¨æ€è¶…æ—¶æ¥åˆ¤æ–­æ˜¯å¦ç»“æŸ ---
	if (now - sentinelState.recordingStartTime >= CONFIG.timing.MAX_RECORDING_MS) {
		console.log('ğŸ•’ Maximum recording duration reached.')
		await finishRecordingSession(now)
	} else if (now - sentinelState.lastLoudTime >= effectiveTimeoutMs) {
		console.log(`ğŸ”‡ Continuous silence detected (dynamic timeout: ${(effectiveTimeoutMs / 1000).toFixed(2)}s).`)
		await finishRecordingSession(now)
	}
}

// --- ä¸»ç¨‹åº (Main Application) ---

async function restartRecorder() {
	if (sentinelState.recorder) sentinelState.recorder.release()
	await new Promise(resolve => setTimeout(resolve, 1000))

	try {
		sentinelState.recorder = new PvRecorder(CONFIG.audio.FRAME_LENGTH, -1)
		sentinelState.recorder.start()
		console.log('âœ… Recorder restarted.')
		sentinelState.recorderRetryCount = 0
		return true
	} catch (err) {
		console.error(`âŒ Failed to restart recorder: ${err.message}.`)
		sentinelState.recorderRetryCount++
		return false
	}
}

function updateCliDisplay(rms) {
	const bar = 'â–ˆ'.repeat(Math.min(CONFIG.display.CLI_BAR_MAX_LENGTH, Math.floor(rms / CONFIG.display.CLI_BAR_RMS_SCALING_FACTOR)))
	let statusDisplay = `[çŠ¶æ€: ${sentinelState.state.padEnd(16)}] RMS: ${rms.toFixed(0).padEnd(5)} | ${bar}`

	if (sentinelState.state === 'ARMED' && sentinelState.consecutiveLoudFrames > 0)
		statusDisplay += ` (Loud streak: ${sentinelState.consecutiveLoudFrames}/${CONFIG.triggers.CONSECUTIVE_LOUD_FRAMES})`
	else if (sentinelState.state === 'RECORDING') {
		const { currentRecordingStats, recordingStartTime } = sentinelState
		const recordingDuration = ((Date.now() - recordingStartTime) / 1000).toFixed(1)
		const ratio = currentRecordingStats.totalLoudFrames > 0
			? (currentRecordingStats.matchingLoudFrames / currentRecordingStats.totalLoudFrames).toFixed(2) : 'N/A'
		statusDisplay += ` (Recording: ${recordingDuration}s | Match: ${ratio})`
	}
	console.freshLine('status-line', statusDisplay)
}

let isRunning = false

async function sentinelLoop() {
	while (isRunning) {
		let frame
		try {
			if (!sentinelState.recorder) {
				console.error('âŒ Recorder not initialized.')
				await new Promise(resolve => setTimeout(resolve, 5000))
				continue
			}
			frame = await sentinelState.recorder.read()
		} catch (error) {
			if (!isRunning) return
			console.error(`âŒ Failed to read audio frame: ${error.message}.`)
			if (sentinelState.recorderRetryCount < 5) {
				console.log('ğŸ•’ Attempting to restart recorder in 1 minute...')
				await new Promise(resolve => setTimeout(resolve, 60 * 1000))
				console.log('â³ Attempting to restart recorder...')
				if (!await restartRecorder()) continue
			} else {
				console.error('âŒ Recorder restart failed multiple times, stopping sentinel.')
				isRunning = false
			}
			continue
		}

		const now = Date.now()
		const rms = calculateRMS(frame)
		const frameData = { buffer: frame, rms }

		// updateCliDisplay(rms)

		switch (sentinelState.state) {
			case 'INITIALIZING':
				sentinelState.initRmsList.push(rms)
				handleInitializingState(now)
				break
			case 'MONITORING_QUIET':
				handleMonitoringQuietState(rms, now)
				break
			case 'ARMED':
				handleArmedState(frameData, now)
				break
			case 'RECORDING':
				await handleRecordingState(frameData, now)
				break
		}
	}
}

export function stopVoiceSentinel() {
	if (!isRunning) return
	console.log('ğŸ‘‹ Shutting down audio sentinel...')
	isRunning = false
	if (sentinelState.recorder) {
		sentinelState.recorder.release()
		sentinelState.recorder = null
	}
	console.log('ğŸ¤ Audio sentinel stopped.')
}

export async function checkVoiceSentinel() {
	// Stop conditions
	if (!existsSync(CONFIG.files.REFERENCE_WAV)) {
		stopVoiceSentinel()
		return isRunning
	}

	// If running, check for updates
	if (isRunning)
		try {
			const { mtime } = statSync(CONFIG.files.REFERENCE_WAV)
			// If mtime is newer, reload the reference MFCCs
			if (sentinelState.referenceFileMtime && mtime.getTime() > sentinelState.referenceFileMtime.getTime()) {
				console.log('ğŸ”„ Detected voice reference file update, reloading features...')
				sentinelState.referenceMfccs = loadReferenceMfcc() // This also updates the mtime in the state
			}
		} catch (err) {
			console.error('âŒ Error checking voice reference file update, stopping sentinel:', err.message)
			stopVoiceSentinel()
		}
	else
		// If not running, start it
		startVoiceSentinel()

	return isRunning
}

function startVoiceSentinel() {
	if (charConfig.disable_voice_sentinel) {
		console.log('ğŸ¤ Audio sentinel is disabled by config.')
		return
	}
	if (isRunning) {
		console.log('ğŸ¤ Audio sentinel already running.')
		return
	}

	// --- å‰ç½®æ¡ä»¶æ£€æŸ¥ ---
	if (!existsSync(CONFIG.files.REFERENCE_WAV)) {
		console.log(`ğŸ¤ Audio sentinel: Reference audio (${CONFIG.files.REFERENCE_WAV}) does not exist, cannot start.`)
		return
	}

	console.log('ğŸš€ Starting audio sentinel...')
	sentinelState = createInitialState() // é‡ç½®çŠ¶æ€
	isRunning = true

	try {
		sentinelState.recorder = new PvRecorder(CONFIG.audio.FRAME_LENGTH, -1)
		sentinelState.recorder.start()
		console.log(`ğŸ¤ PvRecorder microphone source created (sample rate: ${sentinelState.recorder.sampleRate}).`)
		console.log(`ğŸ”¬ Initializing... Analyzing ambient noise (${CONFIG.timing.INITIALIZATION_MS / 1000} seconds)...`)
		sentinelLoop().catch(err => {
			console.error('âŒ Sentinel main loop exited with error:', err)
			stopVoiceSentinel()
		})
	} catch (err) {
		console.error(`âŒ Failed to start recorder: ${err.message}`)
		isRunning = false
	}
}

/**
 * åˆå§‹åŒ–è¯­éŸ³ç›‘è§†å™¨ã€‚
 * æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯­éŸ³ç›‘è§†å™¨ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„è¯­éŸ³ç›‘è§†å™¨ã€‚
 */
export function initializeVoiceSentinel() {
	initRealityChannel()
	startVoiceSentinel()
}
